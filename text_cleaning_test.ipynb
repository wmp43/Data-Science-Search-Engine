{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4503be14-33c7-4474-aab9-d6f1493fc768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "def fetch_article_content(article_title):\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"titles\": article_title,\n",
    "        \"explaintext\": True,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    url_title = article_title\n",
    "    response = requests.get(url=URL, params=PARAMS)\n",
    "    data = response.json()\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    \n",
    "    # Since there's typically one page per title, you can get the page ID like this\n",
    "    page_id = next(iter(pages))\n",
    "    content = pages[page_id].get(\"extract\", \"\")\n",
    "    title = pages[page_id].get(\"title\", \"\")\n",
    "    wiki_url = f\"https://en.wikipedia.org/wiki/{url_title}\"\n",
    "    return title, page_id, content\n",
    "\n",
    "\n",
    "def tokenize_and_clean(text):\n",
    "    # Tokenize by whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove unwanted LaTeX commands or mathematical expressions from tokens\n",
    "    cleaned_tokens = [token for token in tokens if not re.match(r'\\\\[a-zA-Z]+', token)]\n",
    "    #cleaned_tokens = [token for token in tokens if not re.match(r'({.*?})|(\\\\[a-zA-Z]+)', token)]\n",
    "\n",
    "    \n",
    "    # Rejoin the cleaned tokens\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    #cleaned_text = re.sub(r\"({.*?})\",\"\", clean_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_nested_curly_braces(text):\n",
    "    stack = []\n",
    "    to_remove = []\n",
    "    text_list = list(text)\n",
    "\n",
    "    for i, char in enumerate(text_list):\n",
    "        if char == '{':\n",
    "            stack.append(i - 1) \n",
    "        elif char == '}':\n",
    "            if stack:\n",
    "                start = stack.pop()\n",
    "                if not stack:\n",
    "                    to_remove.append((start, i))\n",
    "\n",
    "    for start, end in reversed(to_remove):\n",
    "        del text_list[start:end + 1]\n",
    "\n",
    "    return ''.join(text_list)\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    # Replace \\n and \\t with an empty string\n",
    "    cleaned_text = re.sub(r'[\\n\\t]+', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "SECTIONS_TO_IGNORE = [\n",
    "    \"See also\",\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"Further reading\",\n",
    "    \"Footnotes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "    \"Citations\",\n",
    "    \"Literature\",\n",
    "    \"Footnotes\",\n",
    "    \"Notes and references\",\n",
    "    \"Photo gallery\",\n",
    "    \"Works cited\",\n",
    "    \"Photos\",\n",
    "    \"Gallery\",\n",
    "    \"Notes\",\n",
    "    \"References and sources\",\n",
    "    \"References and notes\",\n",
    "]\n",
    "\n",
    "def split_and_filter_sections(article_text):\n",
    "    # Split the article into sections using regex pattern\n",
    "    sections = re.split(r'(==\\s*[^=]+\\s*==)', article_text)\n",
    "    # Pair up the section titles with their content\n",
    "    sections = list(zip(sections[1::2], sections[2::2]))\n",
    "    \n",
    "    # Filter out the sections to ignore\n",
    "    filtered_sections = {title.strip('= \\n'): content.strip()\n",
    "                         for title, content in sections\n",
    "                         if title.strip('= \\n') not in SECTIONS_TO_IGNORE}\n",
    "    \n",
    "    return filtered_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00d114f-3bfa-4a5e-a356-eb3d306fced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import requests\n",
    "\n",
    "class HuggingFaceAPI(BaseModel):\n",
    "    \"\"\"\n",
    "    Llama model deployed to HF for text cleaning and summarization\n",
    "\n",
    "    \"\"\"\n",
    "    token: str\n",
    "    endpoint: str\n",
    "\n",
    "    def fetch_summary(self, text_chunk: str) -> str:\n",
    "        \"\"\"\n",
    "        :param text_chunk: Chunk of tokenized text that is going to get summarized\n",
    "        :return: summary str\n",
    "        \"\"\"\n",
    "        headers = {\"Authorization\": f\"Bearer {hf_token}\", \"Content-Type\": \"application/json\"}\n",
    "        payload = {\"inputs\": f\"<s>[INST] <<SYS>> Remove all latex and equations. Do Not summarize<</SYS>> {text_chunk}[/INST]</s>\",\n",
    "                  \"parameters\": {\"max_new_tokens\": 500, \"top_k\": 40}}\n",
    "    \n",
    "        response = requests.post(self.endpoint, headers=headers, data=json.dumps(payload))\n",
    "        response_json = response.json()\n",
    "        print\n",
    "        return response_json[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa44587d-8ee1-4792-86db-8f91082a668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "dict_keys(['Definitions', 'Standard normal distribution', 'General normal distribution', 'Notation', 'Alternative parameterizations', 'Cumulative distribution function', 'Error Function', 'Recursive computation with Taylor series expansion', \"Using the Taylor series and Newton's method for the inverse function\", 'Standard deviation and coverage', 'Quantile function', 'Properties', 'Symmetries and derivatives', 'Moments', 'Fourier transform and characteristic function', 'Moment- and cumulant-generating functions', 'Stein operator and class', 'Zero-variance limit', 'Maximum entropy', 'Other properties', 'Related distributions', 'Central limit theorem', 'Operations and functions of normal variables', 'Operations on a single normal variable', 'Operations on two independent normal variables', 'Operations on two independent standard normal variables', 'Operations on multiple independent normal variables', \"Any linear combination of independent normal deviates is a normal deviate. If X 1 , X 2 , … , X n {\\\\displaystyle X_{1},X_{2},\\\\ldots ,X_{n}} are independent standard normal random variables, then the sum of their squares has the chi-squared distribution with n {\\\\displaystyle n} degrees of freedom If X 1 , X 2 , … , X n {\\\\displaystyle X_{1},X_{2},\\\\ldots ,X_{n}} are independent normally distributed random variables with means μ {\\\\displaystyle } and variances σ 2 {\\\\displaystyle ^{2}} , then their sample mean is independent from the sample standard deviation, which can be demonstrated using Basu's theorem or Cochran's theorem. The ratio of these two quantities will have the Student's t-distribution with n − 1 {\\\\displaystyle n-1} degrees of freedom: If X 1 , X 2 , … , X n {\\\\displaystyle X_{1},X_{2},\\\\ldots ,X_{n}} , Y 1 , Y 2 , … , Y m {\\\\displaystyle Y_{1},Y_{2},\\\\ldots ,Y_{m}} are independent standard normal random variables, then the ratio of their normalized sums of squares will have the F-distribution with (n, m) degrees of freedom:\", 'Operations on multiple correlated normal variables', 'Operations on the density function', \"Infinite divisibility and Cramér's theorem\", \"Bernstein's theorem\", 'Extensions', 'Statistical inference', 'Estimation of parameters', 'Sample mean', 'Estimator μ ^ {\\\\displaystyle {\\\\hat {\\\\mu }}} is called the sample mean, since it is the arithmetic mean of all observations. The statistic x ¯ {\\\\displaystyle {\\\\overline {x}}} is complete and sufficient for μ {\\\\displaystyle } , and therefore by the Lehmann–Scheffé theorem, μ ^ {\\\\displaystyle {\\\\hat {\\\\mu }}} is the uniformly minimum variance unbiased (UMVU) estimator. In finite samples it is distributed normally: μ ^ ∼ N ( μ , σ 2 / n ) . {\\\\displaystyle {\\\\hat {\\\\mu }}\\\\sim {\\\\mathcal {N}}(\\\\mu ,\\\\sigma ^{2}/n).} The variance of this estimator is equal to the μμ-element of the inverse Fisher information matrix I − 1 {\\\\displaystyle {\\\\mathcal {I}}^{-1}} . This implies that the estimator is finite-sample efficient. Of practical importance is the fact that the standard error of μ ^ {\\\\displaystyle {\\\\hat {\\\\mu }}} is proportional to 1 / n {\\\\displaystyle 1/{\\\\sqrt {n}}} , that is, if one wishes to decrease the standard error by a factor of 10, one must increase the number of points in the sample by a factor of 100. This fact is widely used in determining sample sizes for opinion polls and the number of trials in Monte Carlo simulations. From the standpoint of the asymptotic theory, μ ^ {\\\\displaystyle {\\\\hat {\\\\mu }}} is consistent, that is, it converges in probability to μ {\\\\displaystyle } as n → ∞ {\\\\displaystyle n\\\\rightarrow } . The estimator is also asymptotically normal, which is a simple corollary of the fact that it is normal in finite samples: n ( μ ^ − μ ) → d N ( 0 , σ 2 ) . {\\\\displaystyle {\\\\sqrt {n}}({\\\\hat {\\\\mu }}-\\\\mu )\\\\,{\\\\xrightarrow {d}}\\\\,{\\\\mathcal {N}}(0,\\\\sigma ^{2}).}', 'Sample variance', 'Confidence intervals', 'Normality tests', 'Bayesian analysis of the normal distribution', 'Sum of two quadratics', '', 'Scalar form', 'Vector form', 'Sum of differences from the mean', 'With known variance', 'With known mean', 'With unknown mean and unknown variance', 'Occurrence and applications', 'Exact normality', 'Approximate normality', 'Assumed normality', 'Methodological problems and peer review', 'Computational methods', 'Generating values from normal distribution', 'Numerical approximations for the normal cumulative distribution function and normal quantile function', 'History', 'Development', 'Naming'])\n"
     ]
    }
   ],
   "source": [
    "title, page_id, content = fetch_article_content('Normal_distribution')\n",
    "cleaned_text = tokenize_and_clean(content)\n",
    "split_secs = split_and_filter_sections(cleaned_text)\n",
    "print(len(split_secs.keys()))\n",
    "pp.pprint(split_secs.keys())\n",
    "\n",
    "hf_text = cleaned_text[:2253]\n",
    "#print(remove_nested_curly_braces(hf_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7e5a6f-380f-4053-90cb-b83cb243bd73",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m hf_token, hf_endpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_fQnZVUvlBHkgQYNYUWTsiQgCgMedrKPPiN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://rni9jdayjgc6ea8b.us-east-1.aws.endpoints.huggingface.cloud\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m      2\u001b[0m hf_api \u001b[38;5;241m=\u001b[39m HuggingFaceAPI(token\u001b[38;5;241m=\u001b[39mhf_token, endpoint\u001b[38;5;241m=\u001b[39mhf_endpoint)\n\u001b[0;32m----> 3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mhf_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m, in \u001b[0;36mHuggingFaceAPI.fetch_summary\u001b[0;34m(self, text_chunk)\u001b[0m\n\u001b[1;32m     22\u001b[0m response_json \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse_json\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "hf_token, hf_endpoint = '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', 'https://rni9jdayjgc6ea8b.us-east-1.aws.endpoints.huggingface.cloud' \n",
    "hf_api = HuggingFaceAPI(token=hf_token, endpoint=hf_endpoint)\n",
    "text = hf_api.fetch_summary(hf_text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40dd914a-b1ec-4762-8dd1-229d25d5bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import mwclient\n",
    "import mwparserfromhell\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b9c99e-0b96-443e-a6b5-16dfee8f8cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 732 article titles in Category:2022 Winter Olympics.\n"
     ]
    }
   ],
   "source": [
    "# get Wikipedia pages about the 2022 Winter Olympics\n",
    "\n",
    "CATEGORY_TITLE = \"Category:2022 Winter Olympics\"\n",
    "WIKI_SITE = \"en.wikipedia.org\"\n",
    "\n",
    "\n",
    "def titles_from_category(\n",
    "    category: mwclient.listing.Category, max_depth: int\n",
    ") -> set[str]:\n",
    "    \"\"\"Return a set of page titles in a given Wiki category and its subcategories.\"\"\"\n",
    "    titles = set()\n",
    "    for cm in category.members():\n",
    "        if type(cm) == mwclient.page.Page:\n",
    "            # ^type() used instead of isinstance() to catch match w/ no inheritance\n",
    "            titles.add(cm.name)\n",
    "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:\n",
    "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n",
    "            titles.update(deeper_titles)\n",
    "    return titles\n",
    "\n",
    "\n",
    "site = mwclient.Site(WIKI_SITE)\n",
    "category_page = site.pages[CATEGORY_TITLE]\n",
    "titles = titles_from_category(category_page, max_depth=1)\n",
    "# ^note: max_depth=1 means we go one level deep in the category tree\n",
    "print(f\"Found {len(titles)} article titles in {CATEGORY_TITLE}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d443edb9-b85b-4f64-9820-d1390694e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to split Wikipedia pages into sections\n",
    "\n",
    "SECTIONS_TO_IGNORE = [\n",
    "    \"See also\",\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"Further reading\",\n",
    "    \"Footnotes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "    \"Citations\",\n",
    "    \"Literature\",\n",
    "    \"Footnotes\",\n",
    "    \"Notes and references\",\n",
    "    \"Photo gallery\",\n",
    "    \"Works cited\",\n",
    "    \"Photos\",\n",
    "    \"Gallery\",\n",
    "    \"Notes\",\n",
    "    \"References and sources\",\n",
    "    \"References and notes\",\n",
    "]\n",
    "\n",
    "\n",
    "def all_subsections_from_section(\n",
    "    section: mwparserfromhell.wikicode.Wikicode,\n",
    "    parent_titles: list[str],\n",
    "    sections_to_ignore: set[str],\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"\n",
    "    From a Wikipedia section, return a flattened list of all nested subsections.\n",
    "    Each subsection is a tuple, where:\n",
    "        - the first element is a list of parent subtitles, starting with the page title\n",
    "        - the second element is the text of the subsection (but not any children)\n",
    "    \"\"\"\n",
    "    headings = [str(h) for h in section.filter_headings()]\n",
    "    title = headings[0]\n",
    "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
    "        # ^wiki headings are wrapped like \"== Heading ==\"\n",
    "        return []\n",
    "    titles = parent_titles + [title]\n",
    "    full_text = str(section)\n",
    "    section_text = full_text.split(title)[1]\n",
    "    if len(headings) == 1:\n",
    "        return [(titles, section_text)]\n",
    "    else:\n",
    "        first_subtitle = headings[1]\n",
    "        section_text = section_text.split(first_subtitle)[0]\n",
    "        results = [(titles, section_text)]\n",
    "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
    "            results.extend(all_subsections_from_section(subsection, titles, sections_to_ignore))\n",
    "        return results\n",
    "\n",
    "\n",
    "def all_subsections_from_title(\n",
    "    title: str,\n",
    "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n",
    "    site_name: str = WIKI_SITE,\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"From a Wikipedia page title, return a flattened list of all nested subsections.\n",
    "    Each subsection is a tuple, where:\n",
    "        - the first element is a list of parent subtitles, starting with the page title\n",
    "        - the second element is the text of the subsection (but not any children)\n",
    "    \"\"\"\n",
    "    site = mwclient.Site(site_name)\n",
    "    page = site.pages[title]\n",
    "    text = page.text()\n",
    "    parsed_text = mwparserfromhell.parse(text)\n",
    "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
    "    if headings:\n",
    "        summary_text = str(parsed_text).split(headings[0])[0]\n",
    "    else:\n",
    "        summary_text = str(parsed_text)\n",
    "    results = [([title], summary_text)]\n",
    "    for subsection in parsed_text.get_sections(levels=[2]):\n",
    "        results.extend(all_subsections_from_section(subsection, [title], sections_to_ignore))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a1a43-656d-4a70-8e4b-7f6021be0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split pages into sections\n",
    "# may take ~1 minute per 100 articles\n",
    "wikipedia_sections = []\n",
    "for title in titles:\n",
    "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
    "print(f\"Found {len(wikipedia_sections)} sections in {len(titles)} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48899659-cce1-4687-93dc-f9ae1a87b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text\n",
    "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
    "    \"\"\"\n",
    "    Return a cleaned up section with:\n",
    "        - <ref>xyz</ref> patterns removed\n",
    "        - leading/trailing whitespace removed\n",
    "    \"\"\"\n",
    "    titles, text = section\n",
    "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
    "    text = text.strip()\n",
    "    return (titles, text)\n",
    "\n",
    "\n",
    "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
    "\n",
    "# filter out short/blank sections\n",
    "def keep_section(section: tuple[list[str], str]) -> bool:\n",
    "    \"\"\"Return True if the section should be kept, False otherwise.\"\"\"\n",
    "    titles, text = section\n",
    "    if len(text) < 16:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "original_num_sections = len(wikipedia_sections)\n",
    "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
    "print(f\"Filtered out {original_num_sections-len(wikipedia_sections)} sections, leaving {len(wikipedia_sections)} sections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f353a-768f-44a7-b024-6cc79bbbaa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print example data\n",
    "for ws in wikipedia_sections[:5]:\n",
    "    print(ws[0])\n",
    "    display(ws[1][:77] + \"...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952806ec-281e-4ae3-b643-dd9e29756f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
    "\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]  # no delimiter found\n",
    "    elif len(chunks) == 2:\n",
    "        return chunks  # no need to search for halfway point\n",
    "    else:\n",
    "        total_tokens = num_tokens(string)\n",
    "        halfway = total_tokens // 2\n",
    "        best_diff = halfway\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            left = delimiter.join(chunks[: i + 1])\n",
    "            left_tokens = num_tokens(left)\n",
    "            diff = abs(halfway - left_tokens)\n",
    "            if diff >= best_diff:\n",
    "                break\n",
    "            else:\n",
    "                best_diff = diff\n",
    "        left = delimiter.join(chunks[:i])\n",
    "        right = delimiter.join(chunks[i:])\n",
    "        return [left, right]\n",
    "\n",
    "\n",
    "def truncated_string(\n",
    "    string: str,\n",
    "    model: str,\n",
    "    max_tokens: int,\n",
    "    print_warning: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string\n",
    "\n",
    "\n",
    "def split_strings_from_subsection(\n",
    "    subsection: tuple[list[str], str],\n",
    "    max_tokens: int = 1000,\n",
    "    model: str = GPT_MODEL,\n",
    "    max_recursion: int = 5,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Split a subsection into a list of subsections, each with no more than max_tokens.\n",
    "    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
    "    \"\"\"\n",
    "    titles, text = subsection\n",
    "    string = \"\\n\\n\".join(titles + [text])\n",
    "    num_tokens_in_string = num_tokens(string)\n",
    "    # if length is fine, return string\n",
    "    if num_tokens_in_string <= max_tokens:\n",
    "        return [string]\n",
    "    # if recursion hasn't found a split after X iterations, just truncate\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "    # otherwise, split in half and recurse\n",
    "    else:\n",
    "        titles, text = subsection\n",
    "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "            if left == \"\" or right == \"\":\n",
    "                # if either half is empty, retry with a more fine-grained delimiter\n",
    "                continue\n",
    "            else:\n",
    "                # recurse on each half\n",
    "                results = []\n",
    "                for half in [left, right]:\n",
    "                    half_subsection = (titles, half)\n",
    "                    half_strings = split_strings_from_subsection(\n",
    "                        half_subsection,\n",
    "                        max_tokens=max_tokens,\n",
    "                        model=model,\n",
    "                        max_recursion=max_recursion - 1,\n",
    "                    )\n",
    "                    results.extend(half_strings)\n",
    "                return results\n",
    "    # otherwise no split was found, so just truncate (should be very rare)\n",
    "    return [truncated_string(string, model=model, max_tokens=max_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eea227-478f-419f-98cc-ddfc91edec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sections into chunks\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e8bc3-0357-455b-93df-3411db3b75e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print example data\n",
    "print(wikipedia_strings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f217f5dd-05b1-4cf3-afc2-e59225688a43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
